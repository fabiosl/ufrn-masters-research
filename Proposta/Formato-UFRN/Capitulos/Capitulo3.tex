% Capitulo 3
\chapter{Validation}\label{validationChap}

To validate our guidelines, we searched for open-source tools that could possibly require database transitions. As stated on section \ref{introductionChap}, a database migration is justified when the alternatives have better performance/manutenability than the classic RDBMSs and/or the cost to have a similar performance on the relational database architecture is significantly higher.

This condition (assessing the need for a database transition) is, then, only verifiable on production or simulated environments. Wordpress\cite{wordpress}, world's most popular Content Management System (CMS)  \cite{cmsranking}, can be used to illustrate this problem.  

Wordpress is built on the top of relational databases, such as PostgreSQL and MySQL. So, to justify a database transition on a wordpress-based application, we must \textit{(i)} have access to a deployed and active version of the CMS or \textit{(ii)} build a simulated environment and load it with posts, pages, comments and other entities that are present on a regular wordpress website.

Having access to the production version of a heavily used tool, such as Wordpress, is not easy, as the owner of the CMS must give access to sensible information of its database, users and posts. 

Other open source tools were considered on this phase of the research, such as Redmine \cite{redmine} and Moodle \cite{moodle}, but the same problem that we had trying to accessing sensible information of Wordpress environments was found on these tools.

Besides that, famous open source tools usually have a large user-base. This results in a large support from the community towards the development of the software. With that, if a database transition from RDBMs to NoSQL is needed, it possibly have plugins/addons from the community, as \cite{fantasticElasticsearch}.

Facing this access restrictions to popular open-source projects, to validate the Guidelines proposed on the previous chapter, we have built scenarios that can be easily mapped to real-world applications.

The application proposed on section~\ref{socmediamonitoringapp} is used to illustrate our case studies.

\section{Assessing database transition on a social media monitoring application}
\label{socmediamonitoringapp}

To validate the guidelines presented on Chapter~\ref{theProblemChap} we have modeled \textit{the core database schema} of a social media monitoring  application. Some examples of industry platforms of this kind are \cite{sproutsocial}, \cite{mention} and \cite{buzzmonitor}. 

This category of applications are capable of searching and storing posts from social media platforms, as Twitter and Facebook. On the setup of a new account, users usually define a boolean query of the terms that they want to monitor and the platform begins to monitor \textit{Application Program Interfaces} (APIs) from social networks, searching publications that match the boolean query. 

\subsection{Application Requirements and use cases}
\label{appoperations}
From a user perspective, this kind of application might be used to search for relevant topics across social media posts, to be aware of public opinion about brands (brand awareness) or to assess market for a product, for example. 

From a software engineering perspective, some use-cases of this application could be:

\begin{enumerate}
\item{\textbf{Create, retrieve, update and delete (CRUD) user account}}
\item{\textbf{CRUD users' terms to monitor}}


\item{\textbf{Retrieve posts by ids} - Given a set of post ids, the user is able to visualize them on the user interface (UI).}

\item{\textbf{Classify posts (add/tags tags)} - \textit{N} tags (subjects) can be added or removed from a set of posts.}

\item{\textbf{Filter captured posts by filters} - Some possible filters are: 
\begin{itemize}
\item{Boolean Query:} Search posts that mention a boolean query string. \textit{i.e: (Brazil AND Neymar) OR (Orlando AND Kaka)}
\item{Date:} Search posts that were published within a date range. For example: posts from 2015/01/01 to 2015/02/01.
\item{Tags:} Search posts that match a tag query. i.e: posts about ``Soccer Player'' and ``High Salary''.
\end{itemize}
}

\end{enumerate}

Other possible features and use-cases of this kind of applications are not listed to ease the understanding of the scenarios that are presented on the following sections.

\subsection{Application Architecture}
Building applications like this one brings some challenges and decisions to software architects and engineers. As the scope of the project grows, it can be a wise decision to split the application in various sub-applications (or components).

Moreover, it is quite costly to manage the whole application from a single code base. The source code needs to handle users registration, query several API's and automatically analyze the sentiment of posts using Natural Language Processing techniques, for example. 

Each of these components may have several other sub-components, or microservices. A microservice architecture brings some benefits to the application, such as maintainability, reuse and simplicity of deployment. A microservice that automatically detects the language from a given a string, for example, can be built and used in all components that need to accomplish this task. 

Components or service-oriented architectures are useful as it enables using different technologies for distinct purposes. Just as in polyglot persistence, the idea is to use the best technology for each task.

Several sub applications could be proposed and created to work with each social network that is supported by the application. i.e: One service/component might be able to handle all the content related to \textit{Facebook}, other component may be responsible for \textit{Twitter} and another one for \textit{Pinterest}-related features. 

On Figure~\ref{fig:apparchitecture} we illustrate a component-based architecture for the proposed application.

\begin{figure}[ht!]
\centering
\includegraphics[width=120mm]{Imagens/apparchitecture.png}
\caption{Proposed architecture - Social Media monitoring app.\label{fig:apparchitecture}}
\end{figure}

Five main components can be extracted from the proposed architecture: 

\begin{itemize}
\item{
\textbf{MainApp Component}: This component can be seen as a web application where users can create their account, manage their social profiles, edit the terms that they want to monitor and where the user-interface is presented. A web framework, such as Ruby on Rails or Django can be used to build this component. 
}

\item{
\textbf{Twitter, Google Plus and Facebook Components}: These components are responsible to provide the necessary communication between the MainApp component and Social Networks. The strategy to build a component for each social network was assumed as different social networks have distinct APIs, with different entities and distinct communication patterns.

In other words, \textit{Facebook} may release its API through a HTTP REST perspective, while \textit{Twitter} may release Software Development Kits (SDKs) to a restrict set of programming languages (Gems for Ruby, JARs for Java, for example) and Google plus may release its API through the SOAP protocol, making XML the standard way to handle data on its scenario.   
}
\item{

\textbf{Adapter Component}: The adapter component is responsible to integrate and to act as an interface between the different schemas that compose the data layer from other components (Facebook, twitter and Google Plus).

}
\end{itemize}

As presented on Figure~\ref{fig:apparchitecture}, the architecture of the proposed application relies a Polyglot Persistence strategy. Two distinct databases are used on the app: Postgresql and MySQL. On real-world scenarios, this situation might be caused because of legacy code, by internal decisions of application architects or any other reason.


\subsection{When application grows, problems may arise}
\label{shithappens}

Once the application is released, it is desirable that no performance issues are found. Operations executed by application users should, theoretically, be at a acceptable performance level.

In the Facebook component, for example, it is expectable to have no performance issues with tens to hundreds of rows stored on the MySQL database. However, worldwide events, as the World Cup or national elections, may start to be suddenly monitored, bringing an unusual workload to the components.  

In these cases, or when the application starts to keep track of millions or billions of data records, application performance and user experience may  be hardly affected if the engineering team have not done sufficient load and stress tests.   

Following this scenario, after some months since application release, the number of records on MySQL databases from the three social-networks components may grow exponentially. A simple query that checked if a column contains a substring starts to analyze millions of posts instead of hundreds.   

Dependendo dos termos cadastrados, depois de alguns meses, o numero de posts nas bases MySQL dos tres subsistemas pode crescer bastante, e uma simples consulta que verificava se um post continha uma substring pode passar a analisar milhoes de posts ao inves de centenas. Intuition tells us that as the number of posts to be analyzed grows, the query time also grows.

\subsection{The application needs other databases?}
\label{anotherdb}
On the scenario described on section~\ref{shithappens}, application users may start to complain that the application is taking too long to process their requests. This situation ends up slowing the overall performance of the users, as an action that could be processed within seconds starts taking minutes to be finish.  

User complainments about performance, however, may be related to a number of different factors. To number a few: the business logic might be taking too long to execute, the number of records in the database may have grown in an unexpected way, the database architecture might not be the best choice to handle application data and failures on network or server hardware may exist. 

Given this scenario, we will focus our analysis on the components that are responsible for storing the posts on MySQL databases. More precisely, our study will be focused on the Facebook component, to present a clearer definition of the problem.

A possible claim from the application users' is that the application is getting slower month after month to filter, process and present the posts on  the user interface (UI). 

It is expected that the number of posts grow month after month. Intuition tells us that the search speed slows down as the number of posts in the database grows, and application developers and DBAs may use this intuition as a hint to find start searching for the real cause of the problem. 

This way, the engineering team from the Facebook component might have a strong intuition that the cause of the slowness is on the data layer (MySQL). Engineers may also have an intuition that it is a specific kind of SQL query that is taking too long to execute, consuming too much CPU \& memory resources. The team may also suggest that a NoSQL technology could be a better fit for the use case that is performing below users' expectation. 

In this case, the guidelines proposed on the previous chapter are a good fit, as they are useful to verify if any problems/bottlenecks exist on the database side and to check if the a NoSQL architecture could be a better fit to the scenario.


\subsection{The application setup: Server}

In this section we detail the server technical specifications that were used to host the data layer of the application that we presented on the previous section. We have selected a \textbf{T2.SMALL} instance from \cite{amazonec2} to host it, as it is a general purpose and low-price instance. T2.SMALL instances feature the following configuration:

\begin{itemize}
\item{High Frequency Intel Xeon Processors with Turbo up to 3.3GHz Burstable CPU, governed by CPU Credits, and consistent baseline performance}
\item{1 vCPU}
\item{12 CPU Credits/hour}
\item{2 GB RAM}
\end{itemize}

\subsection{The application setup: Software}
The following software configuration was installed on the servers: 

\begin{itemize}
\item{Operating System: Ubuntu 14.04.2 LTS (GNU/Linux 3.13.0-48-generic x86\_64)}
\item{Secure Shell}
\item{MySQL Version: 14.14 Distrib 5.5.44, for debian-linux-gnu (x86\_64) using readline 6.3}
\end{itemize}

\subsection{The application setup: Data}
To retrieve the data that is used on the scenarios, we have developed a web crawler that gathers posts from Facebook and store them on our MySQL database. The data was captured from public posts on popular Fan Pages.

A total number of 3.332.534 posts (more than 3 milion posts) were captured and the Dump file with these posts is available on XXXXXXXXXXX. 

\subsection{The application setup: Database schema}
The first database schema was built with the intention of being optimized to production environments, and building \textbf{de}-normalized schemas help to leverage application performance, as revealed by \cite{926306}. 

On Figure~\ref{fig:postsTable} it is possible to view all the columns that compose the \textbf{Posts} table. All post entities, like \textit{message} (content of the post), \textit{link} and \textit{number of likes} are present on this table. 

\begin{figure}[ht!]
\centering
\includegraphics[width=80mm]{postTable.png}
\caption{Posts table.\label{fig:postsTable}}
\end{figure}

Figure~\ref{fig:postsTable} reveals that ``tags'' is a text field on the posts table. By internal convention from the engineering team, a tag of a post can be represented by a string with a defined format: \textbf{\textit{\#username\_tagname\#}}.

A ``tags'' cell might contain several tags. Listing \ref{tag_field_standard_format} gives the representation of a tags cell with multiple tags within: 

\begin{lstlisting}[language=json,firstnumber=1, caption=Tag field standard format, label=tag_field_standard_format]
#username1_tag1##username2_tag1##username3_tag1 
\end{lstlisting}

When the user adds a tag to a given post, the ``tags'' field is concatenated with the given tag. A SQL UPDATE clause example to this situation is given on listing \ref{update_tag_sql}.

\begin{lstlisting}[language=sql,firstnumber=1, caption=Update Tag - SQL, label=update_tag_sql]
UPDATE post set tags=concat(tags, '#username3_tag1#') WHERE id=1; 
\end{lstlisting}

To remove a tag, a ``replace'' operator can be used, as shown on listing 

\begin{lstlisting}[language=sql,firstnumber=1, caption=Remove Tag - SQL, label=remove_tag_sql]
UPDATE post SET tags= REPLACE(tags, '#username3_tag1#', '') WHERE id=1; 
\end{lstlisting}

The decision to store the tags in a field instead of storing it on another database was taken to avoid JOINS between different tables on the application. 

\section{Using the guidelines}
As shown on chapter 3, the guidelines proposed on this work are useful to assess and guide database transitions in production-ready or simulated environments. We may use the guidelines as a good starting point to assess if a database migration to NoSQL is really needed or if the problem can be solved by improving the current DB architecture or if it is not even located on the data layer.

Throughout this section we use the guidelines in a step-by-step strategy, as shown on chapter 3, to motivate and guide a database transition on the application proposed on the previous sections of this chapter. 

As stated previously on~\ref{anotherdb}, we want to check if the MySQL Database from Facebook Component needs to be transitioned, given the users are experiencing a degraded QoS in some parts of the application. 

\subsection{List application operations that are performed on database-level}
According to the guidelines, the first step in a scenario where a database transition is being considered is to ``List application operations that are performed on database level''. 

As the focus of our study is the Facebook component, it makes no sense to consider operations from other components, as ``CRUD user account'' and ``CRUD terms to monitor''. 

This way, from the requirements presented on section~\ref{appoperations}, the operations that are related to the Facebook Component are \textbf{``Retrieve posts by ids''}, \textbf{``Classify posts (add tags)''} and \textbf{``Filter captured posts by filters''}. These are the operations that could possibly demand a transitioning process on the data layer of the component.  


\subsection{Define user-centered SLAs}

The section ~\ref{defineusercenteredslas} reveals that for each operation listed on the previous step, a set of SLAs should be proposed. This way, from interviews and surveys with application stakeholders, this network of SLAs could be proposed to the three operations that are subject to our study: 

\begin{itemize}
	\item{\textbf{Retrieve posts by ids}:
		\begin{itemize}
		\item{Ideal threshold: 3 seconds}
		\item{Tolerable threshold: 10 seconds}
		\item{SLA Delta: 10x}
		\end{itemize}
	}

	\item{\textbf{Classify posts (add/remove tags)}:
		\begin{itemize}
		\item{Ideal threshold: 1.5 second}
		\item{Tolerable threshold: 3 second}
		\item{SLA Delta: 3x}
		\end{itemize}
	}

	\item{\textbf{Filter captured posts by filters}:
		\begin{itemize}
		\item{Ideal threshold: 3 seconds}
		\item{Tolerable threshold: 10 seconds}
		\item{SLA Delta: 3.3x}
		\end{itemize}
	}

	\end{itemize}

\subsection{Define database-centered SLAs}
As shown on previous sections, users and other application stakeholders are capable of identifying SLAs to the use cases of the application. On the other hand, the app engineers are responsible for defining SLAs at a database level. 

These SLAs should be more ``tight'' than the SLAs defined by the users, as several other actions are also necessary to accomplish a use-case. In the context of a web application, for example, authentication tokens, filters, business logic and network delays are some of the factors that affect the execution time of a use case. 

This way, a set of Database-oriented SLAs could be defined to the three proposed use-cases: 

\begin{itemize}
\item{
\textbf{Retrieve posts by id}:
	\begin{itemize}
		\item{Ideal threshold: 1.0 seconds}
		\item{Tolerable threshold: 4 seconds}
		\item{SLA Delta: 4x}
		\item{ROFR: 30\%}
	\end{itemize}
}
\item{

\textbf{Classify posts (add/remove tags)}:
	\begin{itemize}
		\item{Ideal threshold: 0.5 second}
		\item{Tolerable threshold: 2 second}
		\item{SLA Delta: 4x}
		\item{ROFR: 30\%}
	\end{itemize}
}

\item{
	\textbf{Filter captured posts by filters}:
	\begin{itemize}
		\item{Ideal threshold: 2 seconds}
		\item{Tolerable threshold: 6 seconds}
		\item{SLA Delta: 3x}
		\item{ROFR: 15\%}
	\end{itemize}
}
\end{itemize}

Thus, from the ``Filter posts captured by filters'' use case, it can be said that a request to filter posts at database level should take up to two seconds to assure that users won't be degraded performance caused by Database-related problems. If the query takes longer than six seconds to execute, users can get really frustrated about the slowness of the tool and this may have a big negative impact on the Quality of Service. 

This SLA also reveals that there should be no problems if up to 15 \% of the requests are executed between two and six seconds. This outlier data points may can be caused by expected instabilities on cloud providers. 


\subsection{Build database-level SLA log alerts}

Parts of the application proposed on this chapter were implemented to assess the use of the guidelines. In a real-world scenario, log alerts could be completely developed within the source code of the application. Other services, such as New Relic, might also be used for this purpose. 

It's also worth mentioning that the SLA checkers are only useful if database queries and processes are being executed by the users. In a simulated environment, user requests can be simulated by load test tools, as JMeter, or can be coded from scratch.  

In our case study we built scenarios that simulate, at the same time, the requests that are sent by users and part of the part of the source code that analyzes SLA conditions and sends alerts if any SLAs is broken. 

Through the implemented code it is possible to identify if is there any scenarios where the SLAs proposed on the previous chapter are not being fulfilled and consequently if is there any degraded QoS causd by database-related problems.

\textbf{Building the test scenarios}

Three scenarios - one for each operation of the Facebook component - were built to assess wether the SLAs for each operation are being met or not.

Through the following subsections we detail how each script was implemented and discuss the motivation to migrate from the relational architecture to a NoSQL strategy on the proposed application. 


\subsubsection{Retrieve posts by id}

The first operation to be checked is to retrieve a set of posts, given their ids. This operation could called from a screen that presents a list of posts to the users, where they are able to check for details on each post, for example. 

At database level, this operation can be seen as a simple SQL query, as revealed by listing~\ref{retrieve_posts_by_ids_sql}. 

\begin{lstlisting}[language=json,firstnumber=1, caption=SQL Query - Retrieve posts by ids, label=retrieve_posts_by_ids_sql]
SELECT * FROM posts WHERE id IN (1,2,41,13,12903, ... ,435,31)
\end{lstlisting}\label{query01}

On the previous step, we have defined that a database-level SLA for this operation is composed by: 

\begin{itemize}
	\item{Ideal threshold: 1.0 seconds}
	\item{Tolerable threshold: 4 seconds}
	\item{SLA Delta: 4x}
	\item{ROFR: 30\%}
\end{itemize}

To verify if the QoS of this operation is below expected, a test-routine was built according to the following steps: 

\begin{itemize}
\item{Generate a random list of 100 ids between between the range of posts that are stored on the database.} 
\item{Start a thread that opens opnnection to the database and retrieves the ResultSet with the list of posts.}
\item{Wait for a random time between 30 to 300 milliseconds, to reproduce real-world scenario and avoid	query flood on the database at once and start another thread of the same type until a total number of 100 threads are initiated.}
\item{Repeat the steps above for 10 times.}
\end{itemize}

Figures ~\ref{fig:core-execution-01} and ~\ref{fig:core-execution-01.2} present the Java implementation of the given algorithm. \footnote{This algorithm was run from the database server that host the database to assure that the measured time woud not be affected by network or other hardware-related delays.}


\begin{figure}[ht!]
\centering
\includegraphics[width=120mm]{Imagens/core-execution-01-1.png}
\caption{Experiment Setup \label{fig:core-execution-01}}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=120mm]{Imagens/core-execution-01-2.png}
\caption{Main Thread - Run Method \label{fig:core-execution-01.2}}
\end{figure}


Figure~\ref{fig:core-execution-01.3} presents the SLA checkers. As revealed on Chapter 3, alerts are triggered when any query exceeds the tolerable threshold or when the ROFR is over the desired level. We have also defined that a minimum number of 10 operations should be executed before any alert is sent, to avoid unnecessary alerts from unusual situations at the first time that a query is run.

\begin{figure}[ht!]
\centering
\includegraphics[width=120mm]{Imagens/core-execution-01-3.png}
\caption{Main Thread - Run Method \label{fig:core-execution-01.3}}
\end{figure}


Figure~\ref{fig:first_scenario} shows the execution time in milliseconds from the first scenario. On the chart, each line represents an experiment, where 100 queries of the same type as in listing~\ref{retrieve_posts_by_ids_sql} are sent to the MySQL database and the response time is registered.

\begin{figure}[ht!]
\centering
\includegraphics[width=150mm]{Imagens/execution-01.png}
\caption{First Scenario - Retrieve post by ids.\label{fig:first_scenario}}
\end{figure}

As the ideal threshold to this operation states that all operations must be executed in up to 1000 milliseconds (a second), we can assure that the data layer is not a bottleneck to the users on this kind of operation. It is worth noticing that over 1.000 queries were executed on this scenario, and not a single query was over 1/10 of the desired ideal threshold. 

Figure \ref{fig:sla_check} shows the Java implementation of the java thread that makes the requests to the MySQL database and processes them on the SLA checkers.

\begin{figure}[ht!]
\centering
\includegraphics[width=120mm]{Imagens/check_sla.png}
\caption{Check SLA Violation.\label{fig:sla_check}}
\end{figure}

On the Appendix ~\ref{executionreport02} it is possible to visualize the raw data used on this first scenario. 

\subsubsection{Classify posts (add/remove tags) - Not Ready}
> Inicialmente nao funciona, mas mudo o schema das tags e passa a funcionar

\subsubsection{Filter captured posts by filters - Not Ready}
> Tem que transicionar para o Elasticsearch.

To verify this scenario within our database, we have implemented a runnable SLA checker in Python programming language. The algorithm, shown on Figure~\ref{fig:algorithmSLA01} performs fulltext-search operations on our MySQL database and verifies if the SLA is broken in this scenario. 

First, we define several query sizes, and for each query size we generate random strings. Then, a query with of the following style is performed on our MySQL Database: 

 ``SELECT COUNT(*) FROM posts WHERE message like \%myQueryString\%''

For each query size, we repeat the operation with a distinct word for three times (NUMBER\_OF\_ATTEMPTS\_FOR\_EACH\_QUERY\_LENGTH = 3). On our experiments, we found out that there's no noticeable difference between querying random generated ascii-chars and words from the dictionary.

\begin{figure}[ht!]
\centering
\includegraphics[width=100mm]{algorithmSLA01.png}
\caption{Runnable SLA v0.1 .\label{fig:algorithmSLA01}}
\end{figure}	

We have tested the scenario with [100, 1000,10000, 100000, 100000, 1000000, 2000000 and 30000] posts within our dataset. Execution reports shows us that, indeed, the first SLO is broken with a data-size of 1000000 (one million) posts.

Execution reports for each of these dataset values is available on Appendix B.

% \subsection{Guidelines}
% % % The \textbf{first step} of our guidelines states that to propose a database migration, it is necessary to identify that a requirement of the application is broken. In this case, the search speed requirement is broken).

% % % The \textbf{second step} is to implement a runnable SLA that shows that the Service Level Objective is not being fulfilled. The runnable SLA that was explained shows that the Search Operation Time SLO is not being met. 

% % \textbf{Step number three} is to show that the requirement (or SLO) is broken. Execution Report 01, available on Appendix~\ref{executionreport01} shows that Steps 1, 2 and 3 of our guidelines are done. 

% \textbf{The fourth step} of the guidelines is to propose modifications on the relational database. 

% TODO finish here. http://blog.scoutapp.com/articles/2014/12/19/from-mysql-full-text-search-to-elasticsearch reporta algumas opcoes para melhorar a performance de um full text search com MySQL. 

Dentre as opcoes para melhorar o processo de search, ele aponta duas solucoes: (i) ``To support full-text search, we needed to use the MySQL MyISAM storage engine. This has major downsides, the primary one being full table locks: when a table is updated, no other changes to that table can be performed.'' A outra saida era (ii) ``We ended up doing this. It was a fairly simple step and allowed us to switch to the InnoDB engine on the master, eliminating the table lock issues.

This bought us some time, but it wasn't a long-term solution: we basically were rolling our own search and this frequently involved complex queries that third-party search libraries could perform more efficiently. We ended up with massive queries composed of many JOINs plus AND/ORs - these aren't easy to maintain.

Besides query complexity, it's tough to beat the performance of a dedicated search solution. Our tables have considerable update activity, so this would result in sometimes-significant performance issues.
''
Outros contra-pontos para implementar fulltext search em MySQL apontado por XXX, XXX and XXX sao X Y e Z. Falta terminar essa parte.

\subsection{Verify SLA violation - Not ready}

Para resolver o problema apontado no passo anterior, movemos a estrutura das tags para uma tabela separada. 

On Figure~\ref{fig:tagTable} it is possible to verify the elements that compose a \textbf{Tag} record. Each tag has a unique ID - \textit{idtags}, a \textit{post\_id} (foreign key to the post table), a \textit{tag\_user} (the username of the user who has tagged this post) and \textit{tag\_name}, the subject of this post. 

The Tags table is also denormalized, as \textit{tag\_user} and \textit{tag\_name} are entities that should be represented on separate tables on a normalized schema.

\begin{figure}[ht!]
\centering
\includegraphics[width=60mm]{tagTable.png}
\caption{Tags table.\label{fig:tagTable}}
\end{figure}

\textbf{Tags} information is purposely redundant. This way, it is possible to retrieve the tags of a post from a \textbf{JOIN} query between the Tags and Post tables and by searching it within the \textit{tags} column of the Posts table. This data-redundancy was planned to test which option has better performance on a production scenario.  


\subsection{Propose architectural changes at database-level - Not ready}

Apache Solr, Lucene, Amazon Cloudsearch and Elasticsearch are \textbf{Search Engines} that provide fulltext-search as master-features. 

Elasticsearch (a.k.a. Elastic) was seemed to be a good alternative to the problem that we were facing with MySQL. Some discussion forums were consulted and comparisons and studies were considered on our research \cite{StackOverflowElastic} \cite{SolrVsES} \cite{quoraES}. With that, \textbf{step number four} is done. 

A new data model should be proposed once the new database technology is chosen. As Elasticsearch stores data as JSON documents, the same structure of post that we had on the posts table was transformed into a valid JSON document.

A new server with the same configuration of the one presented on the beginning of this chapter was provisioned and Elasticsearch[1.7.2] version was installed.

To dump the data from MySQL and import to Elasticsearch, a Python script was made \cite{mysqltoes}. However, loading data was taking too long as the script didn't paralellize the bulk insert queries on Elasticsearch and database connection kept dropping. To overcome this situation,an open-source project that connects to MySQL via JDBC and imports data into Elasticsearch \cite{elasticjdbc} was used. 
With that, \textbf{the sixth step} is successfully executed. 

A new runnable SLA was necessary to compare the execution time of the previous database architecture (MySQL) and the one that was proposed. We have joined both runnable SLAs into a single script, available on \cite{runnablesla01}, \textbf{finishing the seventh step}.

It is possible to compare the Execution Report of both database architectures on Execution Report 02 (Appendix \ref{executionreport01}).

With that, it is possible to show that the runnable SLA on the new architecture with all posts results in a significant performance improvement, proving that a database transition may be suitable to this scenario, \textbf{finishing Step 08}.


\subsection{Map current schema \& data on the proposed DB architecture}

\subsection{Process all DB operations from a historical point}


% To proceed with a DB migration, Scenarios 02 and 03 must also be analyzed. 

% \abrv[UFRN -- Universidade Federal do Rio Grande do Norte]{UFRN}

% \section{Scenario 03}
% Update by query Scenario. 

% Elasticsearch doesn't support natively. 

% Update by query plugin

% MySQL wins

% Dizer tambem que o pessoal recomenda migrar pedacos de feature a feature, como nesse case do Coursera. https://tech.coursera.org/blog/2014/09/23/courseras-adoption-of-cassandra/
